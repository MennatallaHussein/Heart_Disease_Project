# â¤ï¸ Heart Disease Prediction & Analysis â€” Full ML Pipeline

![Python](https://img.shields.io/badge/Python-3.11-blue?logo=python)
![Streamlit](https://img.shields.io/badge/Streamlit-UI-orange?logo=streamlit)
![Scikit-learn](https://img.shields.io/badge/Scikit--learn-ML-green?logo=scikit-learn)
![XGBoost](https://img.shields.io/badge/XGBoost-Boost-yellow?logo=xgboost)

---

## ğŸš€ Quick Overview

Predict and analyze **heart disease risks** with a full **Machine Learning pipeline** â€” from preprocessing to Streamlit deployment.


---

## ğŸ¯ Features

- ğŸ§¹ Data Cleaning & Preprocessing
- ğŸ“‰ Dimensionality Reduction (PCA)
- ğŸ¤– Supervised Classification Models: Logistic Regression, Decision Trees, Random Forest, SVM
- ğŸ” Unsupervised Clustering: K-Means & Hierarchical
- âš™ï¸ Hyperparameter Tuning with GridSearchCV / RandomizedSearchCV
- ğŸ’» Streamlit Web UI for interactive predictions
- ğŸŒ Public deployment via Ngrok

---

## ğŸ›  Tools & Libraries

- Python ğŸ | Pandas | NumPy | Matplotlib | Seaborn
- Scikit-learn | XGBoost | RFE | PCA
- Streamlit | Joblib | Pickle

---

## ğŸ—‚ File Structure

```
Heart_Disease_Project/
â”‚â”€â”€ data/
â”‚ â””â”€â”€ heart_disease.csv
â”‚â”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_data_preprocessing.ipynb
â”‚ â”œâ”€â”€ 02_pca_analysis.ipynb
â”‚ â”œâ”€â”€ 03_feature_selection.ipynb
â”‚ â”œâ”€â”€ 04_supervised_learning.ipynb
â”‚ â”œâ”€â”€ 05_unsupervised_learning.ipynb
â”‚ â””â”€â”€ 06_hyperparameter_tuning.ipynb
â”‚â”€â”€ models/
â”‚ â””â”€â”€ final_model.pkl
â”‚â”€â”€ ui/
â”‚ â””â”€â”€ app.py
â”‚â”€â”€ deployment/
â”‚ â””â”€â”€ ngrok_setup.txt
â”‚â”€â”€ results/
â”‚ â””â”€â”€ evaluation_metrics.txt
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ .gitignore
```
---


## ğŸ“Œ Dataset

We use the **UCI Heart Disease Dataset**, which contains patient health attributes for predicting heart disease.

- [UCI Heart Disease Dataset](https://archive.ics.uci.edu/ml/datasets/heart+disease)


---


### **4ï¸âƒ£ Results / Evaluation**

- Supervised Models:
  - Logistic Regression Accuracy: 90%
  - Random Forest Accuracy: 93%
  - SVM Accuracy: 90%
  - DecisionTree Accuracy: 87%
  - XGB Accuracy: 85%


